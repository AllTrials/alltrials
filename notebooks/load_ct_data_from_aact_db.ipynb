{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using remote aact db load Clinical Trials data\n",
    "For the script top work you have to sign-up for aact database access (free) o their webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# import graph_tool.all as gt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import psycopg2 # This package allows you to connect to PostgreSQL database\n",
    "\n",
    "# Our stuff:\n",
    "from alltrials.etl_utils import column_is_empty, column_contains_text, column_is_numeric, column_is_categorical\n",
    "\n",
    "n_samples = 10000\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database credentials setup\n",
    "CHange to your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set your connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'aact', # Default\n",
    "    'user': 'wesserg', # This is your username on the aact webpage\n",
    "    'password': 'h5p1le4sq', # Tjis password is the password you defined while loging on the aact webpage\n",
    "    'host': 'aact-db.ctti-clinicaltrials.org',\n",
    "    'port': 5432  # Default PostgreSQL port\n",
    "}\n",
    "# Connect to the database\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Connected to the database!\")\n",
    "except (Exception, psycopg2.DatabaseError) as error:\n",
    "    print(\"Error:\", error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ctgov schema tables\n",
    "While there are multiple tables available in the database, we will focus on the ctgov schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the tables in the ctgov schema:\n",
      "['id_information', 'drop_withdrawals', 'reported_event_totals', 'browse_conditions', 'browse_interventions', 'countries', 'design_outcomes', 'overall_officials', 'all_design_outcomes', 'all_facilities', 'all_group_types', 'all_id_information', 'all_intervention_types', 'all_interventions', 'all_keywords', 'reported_events', 'provided_documents', 'interventions', 'search_results', 'documents', 'pending_results', 'retractions', 'baseline_measurements', 'design_groups', 'keywords', 'conditions', 'calculated_values', 'intervention_other_names', 'design_group_interventions', 'mesh_headings', 'links', 'brief_summaries', 'sponsors', 'baseline_counts', 'eligibilities', 'detailed_descriptions', 'designs', 'facility_contacts', 'outcome_analyses', 'responsible_parties', 'outcome_counts', 'study_references', 'result_groups', 'milestones', 'central_contacts', 'facility_investigators', 'result_agreements', 'study_searches', 'studies', 'participant_flows', 'outcome_analysis_groups', 'result_contacts', 'ipd_information_types', 'facilities', 'mesh_terms', 'outcomes', 'outcome_measurements', 'all_browse_conditions', 'all_browse_interventions', 'all_cities', 'all_conditions', 'all_countries', 'all_overall_official_affiliations', 'all_overall_officials', 'all_primary_outcome_measures', 'all_secondary_outcome_measures', 'all_sponsors', 'all_states', 'categories', 'covid_19_studies']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "# Get a list of tables in the ctgov schema\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"\"\"\n",
    "   SELECT table_name\n",
    "   FROM information_schema.tables\n",
    "   WHERE table_schema = 'ctgov';\n",
    "\"\"\")\n",
    "\n",
    "tables = cursor.fetchall()\n",
    "print(\"Here are the tables in the ctgov schema:\")\n",
    "print([table_name[0] for table_name in tables])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an example table from ctgov schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id       nct_id                        name  \\\n",
      "0  65954254  NCT03968679        Head and Neck Cancer   \n",
      "1  65954337  NCT03650322                      Cancer   \n",
      "2  65954583  NCT02098252                         AVM   \n",
      "3  65954667  NCT00719888  Small Lymphocytic Lymphoma   \n",
      "4  65954669  NCT00489307                Solid Tumors   \n",
      "\n",
      "                downcase_name  \n",
      "0        head and neck cancer  \n",
      "1                      cancer  \n",
      "2                         avm  \n",
      "3  small lymphocytic lymphoma  \n",
      "4                solid tumors  \n"
     ]
    }
   ],
   "source": [
    "cursor.execute(f\"\"\"SELECT * FROM ctgov.conditions LIMIT {n_samples}\"\"\")\n",
    "result = cursor.fetchall()\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "# Convert the result to a DataFrame with column names\n",
    "df = pd.DataFrame(result, columns=column_names)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets now access all tables, and combine them using the nct_id as the key.\n",
    "\n",
    "In the intermediate steps we will aslo conduct some data cleanup atempting to constarin the data to columns/tables and rows that seem useful. We are trying to strip off missing data rows, missing data columns, duplicated ids etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing data for each column in a table\n",
    "all_tables_dict = dict()\n",
    "df_list = list()\n",
    "\n",
    "for table_name in tqdm(tables):\n",
    "    # 1. Load the table into a DataFrame\n",
    "    cursor.execute(\n",
    "        f\"\"\"SELECT * FROM ctgov.{table_name[0]} LIMIT 10000\n",
    "\"\"\")\n",
    "    result = cursor.fetchall()\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "\n",
    "    df = pd.DataFrame(result, columns=column_names)\n",
    "    # 2. Check of table has a nct_id column and if it is unique and non-empty. If so, set it as index\n",
    "    if len(df) > 0 and \"nct_id\" in df.columns and df[\"nct_id\"].nunique() == len(df):\n",
    "        df.set_index('nct_id', inplace=True)\n",
    "    else:\n",
    "        continue\n",
    "    if \"id\" in df.columns:\n",
    "        df.drop('id', axis=1, inplace=True)\n",
    "    df.rename({\"name\": table_name[0], \"names\": table_name[0]}, axis=1, inplace=True)\n",
    "\n",
    "    # 3. Check for useful columns\n",
    "    text_columns = []\n",
    "    categorical_columns = []\n",
    "    numerical_columns = []\n",
    "    gibberish_columns = []\n",
    "    mostly_empty_columns = []\n",
    "    for column in df.columns:\n",
    "        if column_is_empty(df[column]):\n",
    "            mostly_empty_columns.append(column)\n",
    "        elif column_is_categorical(df[column]):  # Detect categorical columns\n",
    "            categorical_columns.append(column)\n",
    "        elif column_is_numeric(df[column]):  # Detect numerical columns\n",
    "            numerical_columns.append(column)\n",
    "        elif column_contains_text(df[column]):  # Detect text columns\n",
    "            text_columns.append(column)\n",
    "        else:\n",
    "            gibberish_columns.append(column)\n",
    "\n",
    "    all_tables_dict[table_name[0]] = {'text_columns': text_columns, 'categorical_columns': categorical_columns,\n",
    "                                        'numerical_columns': numerical_columns, 'gibberish_columns': gibberish_columns,\n",
    "                                        'mostly_empty_columns': mostly_empty_columns}\n",
    "    df_list.append(df[categorical_columns + numerical_columns + text_columns])    \n",
    "\n",
    "# Aggregate and clean rows\n",
    "all_tables_df = pd.concat([add_df for add_df in df_list if len(add_df)>0], axis=1)\n",
    "all_tables_df.dropna(axis=0, thresh=int(all_tables_df.shape[1]/5), inplace=True)\n",
    "\n",
    "\n",
    "# Dont forget to close the connection\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wespy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
